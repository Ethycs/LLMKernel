{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Kernel - Chat Mode & Notebook Context Demo\n",
    "\n",
    "This notebook demonstrates the key features of the LLM Kernel, including:\n",
    "- Chat mode for natural conversation\n",
    "- Notebook cells as literal context window\n",
    "- Various LLM integrations and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Setup\n",
    "\n",
    "First, let's check our available models and kernel status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Available LLM Models:\n",
      "  - gpt-4o: gpt-4o ‚úÖ (active)\n",
      "  - gpt-4o-mini: gpt-4o-mini\n",
      "  - gpt-4: gpt-4\n",
      "  - gpt-3.5-turbo: gpt-3.5-turbo\n",
      "  - claude-3-opus: claude-3-opus-20240229\n",
      "  - claude-3-sonnet: claude-3-sonnet-20240229\n",
      "  - claude-3-haiku: claude-3-haiku-20240307\n",
      "  - ollama/llama3: ollama/llama3\n",
      "  - ollama/codellama: ollama/codellama\n",
      "  - ollama/mistral: ollama/mistral\n"
     ]
    }
   ],
   "source": [
    "# Check available LLM models\n",
    "%llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ LLM Kernel Status\n",
      "========================================\n",
      "Active Model: gpt-4o\n",
      "Available Models: 10\n",
      "\n",
      "Conversation History: 0 exchanges\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ContextManager' object has no attribute 'get_window_usage'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check current kernel status\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mllm_status\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\Keytone\\Documents\\GitHub\\LLMKernel\\.pixi\\envs\\notebook\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2504\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2502\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2504\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2506\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2507\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2508\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\Keytone\\Documents\\GitHub\\LLMKernel\\llm_kernel\\magic_commands\\base.py:112\u001b[39m, in \u001b[36mBaseMagics.llm_status\u001b[39m\u001b[34m(self, line)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Context window usage\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.kernel, \u001b[33m'\u001b[39m\u001b[33mcontext_manager\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     window_usage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontext_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_window_usage\u001b[49m()\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mContext Window Usage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_usage\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Token usage estimate\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'ContextManager' object has no attribute 'get_window_usage'"
     ]
    }
   ],
   "source": [
    "# Check current kernel status\n",
    "%llm_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traditional LLM Queries\n",
    "\n",
    "Before we enable chat mode, let's see how traditional queries work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%llm\n",
    "What is the capital of France?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are three interesting facts about Paris:\n",
      "\n",
      "1. **City of Light**: Paris is often referred to as \"La Ville Lumi√®re,\" or \"The City of Light.\" This nickname originally referred to its leading role during the Age of Enlightenment, but it also reflects the city's early adoption of street lighting in the 1820s.\n",
      "\n",
      "2. **Home to Famous Landmarks**: Paris is home to some of the most iconic landmarks in the world, including the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. The Louvre is the largest art museum in the world and houses over 380,000 objects, including the famous painting, the Mona Lisa.\n",
      "\n",
      "3. **Caf√© Culture**: Parisian caf√©s have a rich cultural history and are seen as social hubs. Many famous writers, artists, and philosophers, including Ernest Hemingway and Jean-Paul Sartre, frequented establishments like Caf√© de Flore and Les Deux Magots, contributing to a vibrant artistic and intellectual scene.\n",
      "\n",
      "These facts highlight Paris's cultural significance and its influence on art and philosophy throughout history!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure! Here are three interesting facts about Paris:\\n\\n1. **City of Light**: Paris is often referred to as \"La Ville Lumi√®re,\" or \"The City of Light.\" This nickname originally referred to its leading role during the Age of Enlightenment, but it also reflects the city\\'s early adoption of street lighting in the 1820s.\\n\\n2. **Home to Famous Landmarks**: Paris is home to some of the most iconic landmarks in the world, including the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. The Louvre is the largest art museum in the world and houses over 380,000 objects, including the famous painting, the Mona Lisa.\\n\\n3. **Caf√© Culture**: Parisian caf√©s have a rich cultural history and are seen as social hubs. Many famous writers, artists, and philosophers, including Ernest Hemingway and Jean-Paul Sartre, frequented establishments like Caf√© de Flore and Les Deux Magots, contributing to a vibrant artistic and intellectual scene.\\n\\nThese facts highlight Paris\\'s cultural significance and its influence on art and philosophy throughout history!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%llm\n",
    "Now tell me three interesting facts about that city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chat Mode - The Magic Begins! üéâ\n",
    "\n",
    "Now let's enable chat mode. This does two things:\n",
    "1. Allows you to type naturally in cells without `%%llm`\n",
    "2. Automatically enables notebook context mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: ON\n",
      "üìì Notebook context mode: ON\n",
      "üìù Just type in any cell to chat!\n",
      "üí° Your notebook cells are now the LLM's context window!\n"
     ]
    }
   ],
   "source": [
    "# Enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Natural Conversation\n",
    "\n",
    "Now you can just type in cells! No magic commands needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "Yes, I can see your previous questions about the capital of France and the interesting facts you requested about Paris. How can I assist you further?\n",
      "\n",
      "========================================\n",
      "üí¨ Continue in next cell with %%llm\n"
     ]
    }
   ],
   "source": [
    "Hello! I'm now in chat mode. Can you see my previous questions about France?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "15 * 23 equals 345.\n",
      "\n",
      "========================================\n",
      "üí¨ Continue in next cell with %%llm\n"
     ]
    }
   ],
   "source": [
    "Let's do some math. What's 15 * 23?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "Sure! Adding 100 to the previous response would simply mean providing the information again since it does not involve a numerical value to modify. However, if you meant adding 100 to a specific number or context, could you clarify what you‚Äôd like me to add it to?\n",
      "\n",
      "========================================\n",
      "üí¨ Continue in next cell with %%llm\n"
     ]
    }
   ],
   "source": [
    "Great! Now add 100 to that result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Notebook Context - Your Cells ARE the Context!\n",
    "\n",
    "Let's see what context the LLM actually sees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìì Notebook Context Mode - Showing cells that will be sent to LLM:\n",
      "============================================================\n",
      "\n",
      "[1] USER:\n",
      "# Check available LLM models\n",
      "%llm_models\n",
      "----------------------------------------\n",
      "\n",
      "[2] USER:\n",
      "# Check current kernel status\n",
      "%llm_status\n",
      "----------------------------------------\n",
      "\n",
      "[3] USER:\n",
      "What is the capital of France?\n",
      "----------------------------------------\n",
      "\n",
      "[4] ASSISTANT:\n",
      "'The capital of France is Paris.'\n",
      "----------------------------------------\n",
      "\n",
      "[5] USER:\n",
      "Now tell me three interesting facts about that city.\n",
      "----------------------------------------\n",
      "\n",
      "[6] ASSISTANT:\n",
      "'Sure! Here are three interesting facts about Paris:\\n\\n1. **City of Light**: Paris is often referred to as \"La Ville Lumi√®re,\" or \"The City of Light.\" This nickname originally referred to its leading...\n",
      "----------------------------------------\n",
      "\n",
      "[7] USER:\n",
      "# Enable chat mode\n",
      "%llm_chat on\n",
      "----------------------------------------\n",
      "\n",
      "[8] USER:\n",
      "# Show current context\n",
      "%llm_context\n",
      "----------------------------------------\n",
      "\n",
      "Total messages: 8\n",
      "Estimated tokens: ~330\n"
     ]
    }
   ],
   "source": [
    "# Show current context\n",
    "%llm_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Code Understanding\n",
    "\n",
    "The LLM can see and understand code in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "The code you provided defines a recursive function `fibonacci(n)` that calculates the Fibonacci number at position `n`. It then creates a list called `fib_numbers` containing the first 10 Fibonacci numbers (from 0 to 9) using a list comprehension. \n",
      "\n",
      "When you run this code, the output will be:\n",
      "\n",
      "```\n",
      "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
      "```\n",
      "\n",
      "These numbers are the Fibonacci sequence, where each number is the sum of the two preceding ones. Would you like to know more about the Fibonacci sequence or how to optimize this function?\n",
      "\n",
      "========================================\n",
      "üí¨ Continue in next cell with %%llm\n"
     ]
    }
   ],
   "source": [
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "# Calculate first 10 fibonacci numbers\n",
    "fib_numbers = [fibonacci(i) for i in range(10)]\n",
    "print(fib_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Can you explain the fibonacci function I just wrote? Also, is there a more efficient way to implement it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Model Comparison\n",
    "\n",
    "You can compare responses from different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: OFF\n",
      "üìì Notebook context mode: OFF\n"
     ]
    }
   ],
   "source": [
    "# First, let's turn off chat mode temporarily\n",
    "%llm_chat off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Querying 2 models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd39f9996ddb4f22a1aafc1e645e964f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='\\n            <div style=\"padding: 15px;\">\\n                <h4>GPT-4O-MINI</h4>\\n  ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%llm_compare gpt-4o-mini claude-3-haiku\n",
    "Write a haiku about programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: ON\n",
      "üìì Notebook context mode: ON\n",
      "üìù Just type in any cell to chat!\n",
      "üí° Your notebook cells are now the LLM's context window!\n"
     ]
    }
   ],
   "source": [
    "# Re-enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Context Management\n",
    "\n",
    "You can manage how context is handled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%llm_strategy` not found.\n"
     ]
    }
   ],
   "source": [
    "# Check current context strategy\n",
    "%llm_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìì Notebook context mode: ON\n"
     ]
    }
   ],
   "source": [
    "# You can also manually toggle notebook context mode\n",
    "%llm_notebook_context status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Debugging Support\n",
    "\n",
    "For development, you can enable debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable debugger (for VS Code)\n",
    "# %llm_debug 5678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conversation Continuation\n",
    "\n",
    "The beauty of notebook context is that your entire notebook becomes the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Based on everything we've discussed in this notebook, can you summarize the key features of the LLM Kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Tips and Tricks\n",
    "\n",
    "- In chat mode, regular Python code still works normally\n",
    "- Magic commands (starting with %) are not sent to the LLM\n",
    "- Comments (starting with #) are also not sent to the LLM\n",
    "- Empty cells are ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python comment - won't be sent to LLM\n",
    "x = 42\n",
    "print(f\"The answer is {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What was the value of x in the previous cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Disabling Chat Mode\n",
    "\n",
    "When you're done with chat mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status and disable\n",
    "%llm_chat status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%llm_chat off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The LLM Kernel provides:\n",
    "\n",
    "1. **Chat Mode** - Natural conversation without magic commands\n",
    "2. **Notebook Context** - Your cells ARE the context window\n",
    "3. **Multi-Model Support** - Switch between different LLMs\n",
    "4. **Context Visualization** - See what the LLM sees\n",
    "5. **Smart Filtering** - Ignores magic commands and comments\n",
    "6. **Debugging Support** - VS Code integration\n",
    "\n",
    "The notebook becomes a living conversation where every cell contributes to the context!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
