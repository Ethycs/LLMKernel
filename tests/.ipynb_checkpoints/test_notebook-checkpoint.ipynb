{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Kernel Test Notebook\n",
    "\n",
    "Test the LLM kernel functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from LLM Kernel!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test basic Python execution\n",
    "print(\"Hello from LLM Kernel!\")\n",
    "x = 42\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Available LLM Models:\n",
      "  ‚ö™ gpt-4o\n",
      "  ‚úÖ (active) gpt-4o-mini\n",
      "  ‚ö™ gpt-4\n",
      "  ‚ö™ gpt-3.5-turbo\n",
      "  ‚ö™ claude-3-opus\n",
      "  ‚ö™ claude-3-sonnet\n",
      "  ‚ö™ claude-3-haiku\n",
      "  ‚ö™ llama3\n",
      "  ‚ö™ codellama\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "%llm_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ LLM Kernel Status\n",
      "==================================================\n",
      "Active Model: gpt-4o-mini\n",
      "Available Models: 9\n",
      "Conversation History: 0 exchanges\n",
      "\n",
      "Context Strategy: smart\n",
      "Executed Cells: 2\n",
      "Pinned Cells: 0\n"
     ]
    }
   ],
   "source": [
    "# Check kernel status\n",
    "%llm_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (368385269.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mWhat is 2 + 2?\u001b[39m\n                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Test LLM query\n",
    "%%llm\n",
    "What is 2 + 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Display mode set to: chat\n",
      "üí¨ Started new chat thread: 6cff47df\n",
      "üìù Create a new cell below and use %%llm to start chatting!\n"
     ]
    }
   ],
   "source": [
    "# Switch to chat display mode\n",
    "%llm_display chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Jupyter kernel is like a computer program that runs your code in a Jupyter Notebook. When you write code in a Jupyter Notebook and run it, the notebook sends that code to the kernel, which processes it and sends back the output. Each programming language you can use in Jupyter (like Python, R, or Julia) has its own specific kernel. So, a kernel helps connect the notebook interface with the programming language, allowing you to execute code and see results interactively.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A Jupyter kernel is like a computer program that runs your code in a Jupyter Notebook. When you write code in a Jupyter Notebook and run it, the notebook sends that code to the kernel, which processes it and sends back the output. Each programming language you can use in Jupyter (like Python, R, or Julia) has its own specific kernel. So, a kernel helps connect the notebook interface with the programming language, allowing you to execute code and see results interactively.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%llm\n",
    "Explain what a Jupyter kernel is in simple terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Arial, sans-serif; max-width: 800px;\"><h3>üí¨ LLM Conversation History</h3>\n",
       "            <div style=\"margin: 10px 0; padding: 10px; background: #e3f2fd; border-radius: 10px;\">\n",
       "                <strong>üë§ User:</strong><br>\n",
       "                <pre style=\"white-space: pre-wrap; margin: 5px 0;\">Explain what a Jupyter kernel is in simple terms.\n",
       "</pre>\n",
       "            </div>\n",
       "            \n",
       "                <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; border-radius: 10px;\">\n",
       "                    <strong>ü§ñ gpt-4o-mini:</strong><br>\n",
       "                    <pre style=\"white-space: pre-wrap; margin: 5px 0;\">A Jupyter kernel is like a computer program that runs your code in a Jupyter Notebook. When you write code in a Jupyter Notebook and run it, the notebook sends that code to the kernel, which processes it and sends back the output. Each programming language you can use in Jupyter (like Python, R, or Julia) has its own specific kernel. So, a kernel helps connect the notebook interface with the programming language, allowing you to execute code and see results interactively.</pre>\n",
       "                </div>\n",
       "                </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show conversation history\n",
    "%llm_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cell dependency tracking\n",
    "def greet(name):\n",
    "    return f\"Hello, {name}!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, LLM Kernel!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell depends on the previous one\n",
    "greet(\"LLM Kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Pinned cell 9\n"
     ]
    }
   ],
   "source": [
    "# Pin a cell for context\n",
    "%llm_pin_cell 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set LLM_KERNEL_DEBUG=ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] DEBUG: Using selector: SelectSelector\n",
      "[LLM Kernel] DEBUG: \n",
      "\n",
      "[LLM Kernel] DEBUG: \u001b[92mRequest to litellm:\u001b[0m\n",
      "[LLM Kernel] DEBUG: \u001b[92mlitellm.completion(model='gpt-4o-mini', messages=[{'role': 'user', 'content': 'Explain what a Jupyter kernel is in simple terms.\\n'}, {'role': 'assistant', 'content': 'A Jupyter kernel is like a computer program that runs your code and sends the results back to you. When you use Jupyter Notebook, you write code in \"cells.\" The kernel takes that code, executes it, and returns the output so you can see the results right away.\\n\\nYou can think of the kernel as a helper that understands the programming language you\\'re using (like Python, R, or others). It handles the behind-the-scenes work of running your code, allowing you to focus on writing and experimenting with your ideas.'}, {'role': 'user', 'content': \"Let's test this\\n\"}])\u001b[0m\n",
      "[LLM Kernel] DEBUG: \n",
      "\n",
      "[LLM Kernel] DEBUG: self.optional_params: {}\n",
      "[LLM Kernel] DEBUG: SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] DEBUG: \n",
      "LiteLLM: Params passed to completion() {'model': 'gpt-4o-mini', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Explain what a Jupyter kernel is in simple terms.\\n'}, {'role': 'assistant', 'content': 'A Jupyter kernel is like a computer program that runs your code and sends the results back to you. When you use Jupyter Notebook, you write code in \"cells.\" The kernel takes that code, executes it, and returns the output so you can see the results right away.\\n\\nYou can think of the kernel as a helper that understands the programming language you\\'re using (like Python, R, or others). It handles the behind-the-scenes work of running your code, allowing you to focus on writing and experimenting with your ideas.'}, {'role': 'user', 'content': \"Let's test this\\n\"}], 'thinking': None, 'web_search_options': None}\n",
      "[LLM Kernel] DEBUG: \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "[LLM Kernel] DEBUG: Final returned optional params: {'extra_body': {}}\n",
      "[LLM Kernel] DEBUG: self.optional_params: {'extra_body': {}}\n",
      "[LLM Kernel] DEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-mini', 'combined_model_name': 'openai/gpt-4o-mini', 'stripped_model_name': 'gpt-4o-mini', 'combined_stripped_model_name': 'openai/gpt-4o-mini', 'custom_llm_provider': 'openai'}\n",
      "[LLM Kernel] DEBUG: \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.openai.com/v1/ \\\n",
      "-d '{'model': 'gpt-4o-mini', 'messages': [{'role': 'user', 'content': 'Explain what a Jupyter kernel is in simple terms.\\n'}, {'role': 'assistant', 'content': 'A Jupyter kernel is like a computer program that runs your code and sends the results back to you. When you use Jupyter Notebook, you write code in \"cells.\" The kernel takes that code, executes it, and returns the output so you can see the results right away.\\n\\nYou can think of the kernel as a helper that understands the programming language you\\'re using (like Python, R, or others). It handles the behind-the-scenes work of running your code, allowing you to focus on writing and experimenting with your ideas.'}, {'role': 'user', 'content': \"Let's test this\\n\"}], 'extra_body': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "[LLM Kernel] DEBUG: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'timeout': 600.0, 'files': None, 'idempotency_key': 'stainless-python-retry-ad1b004c-77c4-4975-9fea-5c7a59b3908e', 'json_data': {'messages': [{'role': 'user', 'content': 'Explain what a Jupyter kernel is in simple terms.\\n'}, {'role': 'assistant', 'content': 'A Jupyter kernel is like a computer program that runs your code and sends the results back to you. When you use Jupyter Notebook, you write code in \"cells.\" The kernel takes that code, executes it, and returns the output so you can see the results right away.\\n\\nYou can think of the kernel as a helper that understands the programming language you\\'re using (like Python, R, or others). It handles the behind-the-scenes work of running your code, allowing you to focus on writing and experimenting with your ideas.'}, {'role': 'user', 'content': \"Let's test this\\n\"}], 'model': 'gpt-4o-mini'}, 'extra_json': {}}\n",
      "[LLM Kernel] DEBUG: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "[LLM Kernel] DEBUG: close.started\n",
      "[LLM Kernel] DEBUG: close.complete\n",
      "[LLM Kernel] DEBUG: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "[LLM Kernel] DEBUG: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A3A4443E0>\n",
      "[LLM Kernel] DEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023A396F5D90> server_hostname='api.openai.com' timeout=600.0\n",
      "[LLM Kernel] DEBUG: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023A39ACAF90>\n",
      "[LLM Kernel] DEBUG: send_request_headers.started request=<Request [b'POST']>\n",
      "[LLM Kernel] DEBUG: send_request_headers.complete\n",
      "[LLM Kernel] DEBUG: send_request_body.started request=<Request [b'POST']>\n",
      "[LLM Kernel] DEBUG: send_request_body.complete\n",
      "[LLM Kernel] DEBUG: receive_response_headers.started request=<Request [b'POST']>\n",
      "[LLM Kernel] DEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Jul 2025 05:10:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-e1ijymzf1s1a1bikzu2oab47'), (b'openai-processing-ms', b'1215'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'1218'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199851'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'44ms'), (b'x-request-id', b'req_41d3fc7f829f2383891f3176cb6f0c03'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'959c1e79ffb31748-SJC'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] DEBUG: receive_response_body.started request=<Request [b'POST']>\n",
      "[LLM Kernel] DEBUG: receive_response_body.complete\n",
      "[LLM Kernel] DEBUG: response_closed.started\n",
      "[LLM Kernel] DEBUG: response_closed.complete\n",
      "[LLM Kernel] DEBUG: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 04 Jul 2025 05:10:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-e1ijymzf1s1a1bikzu2oab47', 'openai-processing-ms': '1215', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1218', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199851', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '44ms', 'x-request-id': 'req_41d3fc7f829f2383891f3176cb6f0c03', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '959c1e79ffb31748-SJC', 'content-encoding': 'br', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "[LLM Kernel] DEBUG: request_id: req_41d3fc7f829f2383891f3176cb6f0c03\n",
      "[LLM Kernel] DEBUG: RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-BpSzIDmfXl5XqnQiutZES4uva83Fu\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"Sure! If you have code or a specific task you'd like to try out in a Jupyter Notebook, go ahead and share it with me. I can help you understand how the code works, troubleshoot any issues, or explain concepts related to it. Just let me know what you need!\", \"refusal\": null, \"role\": \"assistant\", \"annotations\": [], \"audio\": null, \"function_call\": null, \"tool_calls\": null}}], \"created\": 1751605856, \"model\": \"gpt-4o-mini-2024-07-18\", \"object\": \"chat.completion\", \"service_tier\": \"default\", \"system_fingerprint\": \"fp_34a54ae93c\", \"usage\": {\"completion_tokens\": 58, \"prompt_tokens\": 136, \"total_tokens\": 194, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}}\n",
      "\n",
      "\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n",
      "[LLM Kernel] DEBUG: selected model name for cost calculation: openai/gpt-4o-mini-2024-07-18\n",
      "[LLM Kernel] DEBUG: Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "[LLM Kernel] DEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-mini-2024-07-18', 'combined_model_name': 'openai/gpt-4o-mini-2024-07-18', 'stripped_model_name': 'gpt-4o-mini-2024-07-18', 'combined_stripped_model_name': 'openai/gpt-4o-mini-2024-07-18', 'custom_llm_provider': 'openai'}\n",
      "[LLM Kernel] DEBUG: selected model name for cost calculation: openai/gpt-4o-mini-2024-07-18\n",
      "[LLM Kernel] DEBUG: model_info: {'key': 'gpt-4o-mini-2024-07-18', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 1.5e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': 7.5e-08, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 7.5e-08, 'output_cost_per_token_batches': 3e-07, 'output_cost_per_token': 6e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': {'search_context_size_low': 30.0, 'search_context_size_medium': 35.0, 'search_context_size_high': 50.0}, 'tpm': None, 'rpm': None}\n",
      "[LLM Kernel] DEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-mini-2024-07-18', 'combined_model_name': 'openai/gpt-4o-mini-2024-07-18', 'stripped_model_name': 'gpt-4o-mini-2024-07-18', 'combined_stripped_model_name': 'openai/gpt-4o-mini-2024-07-18', 'custom_llm_provider': 'openai'}\n",
      "[LLM Kernel] DEBUG: response_cost: 5.52e-05\n",
      "[LLM Kernel] DEBUG: model_info: {'key': 'gpt-4o-mini-2024-07-18', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 1.5e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': 7.5e-08, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 7.5e-08, 'output_cost_per_token_batches': 3e-07, 'output_cost_per_token': 6e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': {'search_context_size_low': 30.0, 'search_context_size_medium': 35.0, 'search_context_size_high': 50.0}, 'tpm': None, 'rpm': None}\n",
      "[LLM Kernel] DEBUG: response_cost: 5.52e-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div style=\"font-family: Arial, sans-serif; max-width: 800px;\">\n",
       "                <div style=\"margin: 10px 0; padding: 10px; background: #e3f2fd; border-radius: 10px;\">\n",
       "                    <strong>üë§ User:</strong><br>\n",
       "                    <pre style=\"white-space: pre-wrap; margin: 5px 0;\">Let&#x27;s test this\n",
       "</pre>\n",
       "                </div>\n",
       "                <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; border-radius: 10px;\">\n",
       "                    <strong>ü§ñ gpt-4o-mini:</strong><br>\n",
       "                    <pre style=\"white-space: pre-wrap; margin: 5px 0;\">Sure! If you have code or a specific task you&#x27;d like to try out in a Jupyter Notebook, go ahead and share it with me. I can help you understand how the code works, troubleshoot any issues, or explain concepts related to it. Just let me know what you need!</pre>\n",
       "                </div>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ To continue this conversation, create a new cell below with:\n",
      "%%llm\n",
      "# Your follow-up question here...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] DEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-mini-2024-07-18', 'combined_model_name': 'openai/gpt-4o-mini-2024-07-18', 'stripped_model_name': 'gpt-4o-mini-2024-07-18', 'combined_stripped_model_name': 'openai/gpt-4o-mini-2024-07-18', 'custom_llm_provider': 'openai'}\n",
      "[LLM Kernel] DEBUG: model_info: {'key': 'gpt-4o-mini-2024-07-18', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 1.5e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': 7.5e-08, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 7.5e-08, 'output_cost_per_token_batches': 3e-07, 'output_cost_per_token': 6e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': {'search_context_size_low': 30.0, 'search_context_size_medium': 35.0, 'search_context_size_high': 50.0}, 'tpm': None, 'rpm': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure! If you have code or a specific task you'd like to try out in a Jupyter Notebook, go ahead and share it with me. I can help you understand how the code works, troubleshoot any issues, or explain concepts related to it. Just let me know what you need!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] DEBUG: Logging Details LiteLLM-Success Call streaming complete\n",
      "[LLM Kernel] DEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-mini-2024-07-18', 'combined_model_name': 'openai/gpt-4o-mini-2024-07-18', 'stripped_model_name': 'gpt-4o-mini-2024-07-18', 'combined_stripped_model_name': 'openai/gpt-4o-mini-2024-07-18', 'custom_llm_provider': 'openai'}\n",
      "[LLM Kernel] DEBUG: model_info: {'key': 'gpt-4o-mini-2024-07-18', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 1.5e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': 7.5e-08, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 7.5e-08, 'output_cost_per_token_batches': 3e-07, 'output_cost_per_token': 6e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': {'search_context_size_low': 30.0, 'search_context_size_medium': 35.0, 'search_context_size_high': 50.0}, 'tpm': None, 'rpm': None}\n"
     ]
    }
   ],
   "source": [
    "%%llm\n",
    "Let's test this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a high-level, interpreted programming language known for its readability and simplicity, making it an excellent choice for beginners as well as experienced developers. It was created by Guido van Rossum and first released in 1991. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\n",
      "\n",
      "Key features of Python include:\n",
      "\n",
      "1. **Readable Syntax**: Python emphasizes code readability and simplicity, allowing developers to write clear and logical code for both small and large-scale projects.\n",
      "\n",
      "2. **Dynamic Typing and Memory Management**: Python is dynamically typed, meaning you don't need to declare the data type of a variable before using it. It also handles memory management automatically through garbage collection.\n",
      "\n",
      "3. **Extensive Standard Library**: Python comes with a vast standard library that provides modules and functions for many common tasks, including file handling, web development, data manipulation, and more.\n",
      "\n",
      "4. **Cross-Platform Compatibility**: Python is available on various operating systems, including Windows, macOS, and Linux, enabling developers to write code that runs on multiple platforms.\n",
      "\n",
      "5. **Strong Community Support**: Python has a large and active community, which means a wealth of resources, libraries, and frameworks are available to assist developers. Some popular frameworks and libraries include Django (web development), Pandas (data analysis), NumPy (numerical computing), TensorFlow (machine learning), and Matplotlib (data visualization).\n",
      "\n",
      "6. **Open Source**: Python is open-source software, meaning its source code is freely available for anyone to use, modify, and distribute.\n",
      "\n",
      "Python is widely used in various domains, such as web development, data analysis, artificial intelligence, scientific computing, automation, and more. Its versatility and ease of use have contributed to its status as one of the most popular programming languages in the world.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            var cell = Jupyter.notebook.insert_cell_below('code');\n",
       "            cell.set_text('%%llm_chat\\n# Continue your conversation here...');\n",
       "            cell.focus_cell();\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Python is a high-level, interpreted programming language known for its readability and simplicity, making it an excellent choice for beginners as well as experienced developers. It was created by Guido van Rossum and first released in 1991. Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\\n\\nKey features of Python include:\\n\\n1. **Readable Syntax**: Python emphasizes code readability and simplicity, allowing developers to write clear and logical code for both small and large-scale projects.\\n\\n2. **Dynamic Typing and Memory Management**: Python is dynamically typed, meaning you don't need to declare the data type of a variable before using it. It also handles memory management automatically through garbage collection.\\n\\n3. **Extensive Standard Library**: Python comes with a vast standard library that provides modules and functions for many common tasks, including file handling, web development, data manipulation, and more.\\n\\n4. **Cross-Platform Compatibility**: Python is available on various operating systems, including Windows, macOS, and Linux, enabling developers to write code that runs on multiple platforms.\\n\\n5. **Strong Community Support**: Python has a large and active community, which means a wealth of resources, libraries, and frameworks are available to assist developers. Some popular frameworks and libraries include Django (web development), Pandas (data analysis), NumPy (numerical computing), TensorFlow (machine learning), and Matplotlib (data visualization).\\n\\n6. **Open Source**: Python is open-source software, meaning its source code is freely available for anyone to use, modify, and distribute.\\n\\nPython is widely used in various domains, such as web development, data analysis, artificial intelligence, scientific computing, automation, and more. Its versatility and ease of use have contributed to its status as one of the most popular programming languages in the world.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%llm_chat \n",
    "What is python?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Is it my turn to guess? I'll say... 5! Is that your number?\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            // Check if we're in classic Jupyter Notebook\n",
       "            if (typeof Jupyter !== 'undefined' && Jupyter.notebook) {\n",
       "                var cell = Jupyter.notebook.insert_cell_below('code');\n",
       "                cell.set_text('%%llm_chat\\n# Continue your conversation here...');\n",
       "                cell.focus_cell();\n",
       "            } \n",
       "            // Check if we're in JupyterLab\n",
       "            else if (typeof window.jupyterlab !== 'undefined') {\n",
       "                // For JupyterLab, we need to use a different approach\n",
       "                const app = window.jupyterlab;\n",
       "                if (app && app.shell && app.shell.currentWidget) {\n",
       "                    const notebook = app.shell.currentWidget.content;\n",
       "                    if (notebook && notebook.model) {\n",
       "                        const cellIndex = notebook.activeCellIndex;\n",
       "                        notebook.model.cells.insert(cellIndex + 1, {\n",
       "                            cell_type: 'code',\n",
       "                            source: '%%llm_chat\\n# Continue your conversation here...',\n",
       "                            metadata: {}\n",
       "                        });\n",
       "                        notebook.activeCellIndex = cellIndex + 1;\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Great! Is it my turn to guess? I'll say... 5! Is that your number?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%llm_chat\n",
    "\n",
    "I'm thinking of a number between one and ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
