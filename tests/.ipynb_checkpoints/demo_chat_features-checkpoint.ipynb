{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Kernel - Chat Mode & Notebook Context Demo\n",
    "\n",
    "This notebook demonstrates the key features of the LLM Kernel, including:\n",
    "- Chat mode for natural conversation\n",
    "- Notebook cells as literal context window\n",
    "- Various LLM integrations and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Setup\n",
    "\n",
    "First, let's check our available models and kernel status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available LLM models\n",
    "%llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current kernel status\n",
    "%llm_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traditional LLM Queries\n",
    "\n",
    "Before we enable chat mode, let's see how traditional queries work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%llm\n",
    "What is the capital of France?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%llm\n",
    "Now tell me three interesting facts about that city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chat Mode - The Magic Begins! ðŸŽ‰\n",
    "\n",
    "Now let's enable chat mode. This does two things:\n",
    "1. Allows you to type naturally in cells without `%%llm`\n",
    "2. Automatically enables notebook context mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Natural Conversation\n",
    "\n",
    "Now you can just type in cells! No magic commands needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hello! I'm now in chat mode. Can you see my previous questions about France?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's do some math. What's 15 * 23?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Great! Now add 100 to that result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Notebook Context - Your Cells ARE the Context!\n",
    "\n",
    "Let's see what context the LLM actually sees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current context\n",
    "%llm_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Code Understanding\n",
    "\n",
    "The LLM can see and understand code in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "# Calculate first 10 fibonacci numbers\n",
    "fib_numbers = [fibonacci(i) for i in range(10)]\n",
    "print(fib_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Can you explain the fibonacci function I just wrote? Also, is there a more efficient way to implement it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Model Comparison\n",
    "\n",
    "You can compare responses from different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's turn off chat mode temporarily\n",
    "%llm_chat off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%llm_compare gpt-4o-mini claude-3-haiku\n",
    "Write a haiku about programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Context Management\n",
    "\n",
    "You can manage how context is handled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current context strategy\n",
    "%llm_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also manually toggle notebook context mode\n",
    "%llm_notebook_context status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Debugging Support\n",
    "\n",
    "For development, you can enable debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable debugger (for VS Code)\n",
    "# %llm_debug 5678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conversation Continuation\n",
    "\n",
    "The beauty of notebook context is that your entire notebook becomes the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Based on everything we've discussed in this notebook, can you summarize the key features of the LLM Kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Tips and Tricks\n",
    "\n",
    "- In chat mode, regular Python code still works normally\n",
    "- Magic commands (starting with %) are not sent to the LLM\n",
    "- Comments (starting with #) are also not sent to the LLM\n",
    "- Empty cells are ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python comment - won't be sent to LLM\n",
    "x = 42\n",
    "print(f\"The answer is {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What was the value of x in the previous cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Disabling Chat Mode\n",
    "\n",
    "When you're done with chat mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status and disable\n",
    "%llm_chat status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%llm_chat off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The LLM Kernel provides:\n",
    "\n",
    "1. **Chat Mode** - Natural conversation without magic commands\n",
    "2. **Notebook Context** - Your cells ARE the context window\n",
    "3. **Multi-Model Support** - Switch between different LLMs\n",
    "4. **Context Visualization** - See what the LLM sees\n",
    "5. **Smart Filtering** - Ignores magic commands and comments\n",
    "6. **Debugging Support** - VS Code integration\n",
    "\n",
    "The notebook becomes a living conversation where every cell contributes to the context!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}