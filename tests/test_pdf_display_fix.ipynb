{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test PDF Display Fix\n",
    "\n",
    "This notebook tests that PDF responses are displayed correctly (not as HTML objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active model: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Check current model\n",
    "%llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: ON\n",
      "üìù Just type in any cell to chat!\n",
      "üí° Your notebook cells are now the LLM's context window!\n",
      "üîÑ Context auto-rescans when you add cells\n"
     ]
    }
   ],
   "source": [
    "# Enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ gpt-4o:\n",
      "----------------------------------------\n",
      "2 + 2 equals 4.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test without PDF first\n",
    "What is 2 + 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded PDF 'the_measure_of_apocalypse.pdf' to OpenAI (file_id: file-RVdkEWKuWzULAbZMhyeqgU)\n",
      "üí° You can now ask questions about this PDF in any cell\n"
     ]
    }
   ],
   "source": [
    "# Upload a PDF\n",
    "# Replace with your PDF path\n",
    "%llm_pdf_native f:\\Keytone\\OneDrive\\LaTex\\Tex\\AI_Research\\dense_humans\\the_measure_of_apocalypse.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ gpt-4o:\n",
      "----------------------------------------\n",
      "To determine what a PDF document is about, you would typically need to extract and analyze its content. However, as an AI language model, I can't directly interact with or analyze specific files unless their content is provided as text input. If you can paste the content or a summary of the document here, I'll be happy to help you understand what it's about!\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now test with PDF\n",
    "What is this document about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: OFF\n"
     ]
    }
   ],
   "source": [
    "# Turn off chat mode to test inline display\n",
    "%llm_chat off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I apologize, but as a text-based AI model, I don't have the capability to directly interact with files or analyze uploaded PDFs. My current functionality is limited to text input and output. However, if you provide specific excerpts or details from the PDF, I can help analyze or summarize the content for you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%llm\n",
    "What are the main topics in the uploaded PDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Results\n",
    "\n",
    "1. In chat mode, responses should appear with a formatted header like:\n",
    "   ```\n",
    "   ü§ñ gpt-4o:\n",
    "   ----------------------------------------\n",
    "   [response text]\n",
    "   ----------------------------------------\n",
    "   ```\n",
    "\n",
    "2. In inline mode, responses should appear as plain text\n",
    "\n",
    "3. No `<IPython.core.display.HTML object>` should appear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
