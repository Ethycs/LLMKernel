{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test PDF Display Fix\n",
    "\n",
    "This notebook tests that PDF responses are displayed correctly (not as HTML objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active model: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Check current model\n",
    "%llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Available LLM Models:\n",
      "  - gpt-4o: gpt-4o ‚úÖ (active)\n",
      "  - gpt-4o-mini: gpt-4o-mini\n",
      "  - gpt-4: gpt-4\n",
      "  - gpt-3.5-turbo: gpt-3.5-turbo\n",
      "  - gpt-4.1: gpt-4.1\n",
      "  - o3: o3\n",
      "  - o3-mini: o3-mini\n",
      "  - claude-3-opus: claude-3-opus-20240229\n",
      "  - claude-3-sonnet: claude-3-sonnet-20240229\n",
      "  - claude-3-haiku: claude-3-haiku-20240307\n",
      "  - claude-opus-4: claude-opus-4-20250514\n",
      "  - claude-sonnet-4: claude-sonnet-4-20250514\n",
      "  - claude-3-5-sonnet: claude-3-5-sonnet-20241022\n",
      "  - ollama/llama3: ollama/llama3\n",
      "  - ollama/codellama: ollama/codellama\n",
      "  - ollama/mistral: ollama/mistral\n"
     ]
    }
   ],
   "source": [
    "%llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: ON\n",
      "üìù Just type in any cell to chat!\n",
      "üí° Your notebook cells are now the LLM's context window!\n",
      "üîÑ Context auto-rescans when you add cells\n"
     ]
    }
   ],
   "source": [
    "# Enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ gpt-4o:\n",
      "----------------------------------------\n",
      "2 + 2 equals 4.\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test without PDF first\n",
    "What is 2 + 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded PDF 'the_measure_of_apocalypse.pdf' to OpenAI (file_id: file-BWir9z64yfT2LccNaviLBP)\n",
      "üí° You can now ask questions about this PDF in any cell\n"
     ]
    }
   ],
   "source": [
    "# Upload a PDF\n",
    "# Replace with your PDF path\n",
    "%llm_pdf_native f:\\Keytone\\OneDrive\\LaTex\\Tex\\AI_Research\\dense_humans\\the_measure_of_apocalypse.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ gpt-4o:\n",
      "----------------------------------------\n",
      "Error: litellm.InternalServerError: OpenAIException - The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_13196557e660765f4315176ae4b343fc in your email.)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now test with PDF\n",
    "What is this document about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: OFF\n"
     ]
    }
   ],
   "source": [
    "# Turn off chat mode to test inline display\n",
    "%llm_chat off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I apologize, but as a text-based AI model, I don't have the capability to directly interact with files or analyze uploaded PDFs. My current functionality is limited to text input and output. However, if you provide specific excerpts or details from the PDF, I can help analyze or summarize the content for you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%llm\n",
    "What are the main topics in the uploaded PDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Results\n",
    "\n",
    "1. In chat mode, responses should appear with a formatted header like:\n",
    "   ```\n",
    "   ü§ñ gpt-4o:\n",
    "   ----------------------------------------\n",
    "   [response text]\n",
    "   ----------------------------------------\n",
    "   ```\n",
    "\n",
    "2. In inline mode, responses should appear as plain text\n",
    "\n",
    "3. No `<IPython.core.display.HTML object>` should appear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
