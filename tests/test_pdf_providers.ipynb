{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Upload Test - All Providers\n",
    "\n",
    "This notebook tests PDF upload functionality across all three major providers:\n",
    "- OpenAI (via Assistants API)\n",
    "- Claude (via Files API)\n",
    "- Gemini (via Files API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration\n",
    "TEST_PDF = r\"F:\\Keytone\\OneDrive\\LaTex\\Tex\\AI_Research\\dense_humans\\the_measure_of_apocalypse.pdf\"\n",
    "\n",
    "# You can change this to test with your own PDF\n",
    "# TEST_PDF = \"path/to/your/document.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test OpenAI (GPT-4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Switched to gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Switch to OpenAI\n",
    "%llm_model gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded PDF 'the_measure_of_apocalypse.pdf' to OpenAI (file_id: file-RVdkEWKuWzULAbZMhyeqgU)\n",
      "üí° You can now ask questions about this PDF in any cell\n"
     ]
    }
   ],
   "source": [
    "# Upload the PDF\n",
    "%llm_pdf_native {TEST_PDF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: ON\n",
      "üìù Just type in any cell to chat!\n",
      "üí° Your notebook cells are now the LLM's context window!\n",
      "üîÑ Context auto-rescans when you add cells\n"
     ]
    }
   ],
   "source": [
    "# Enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                        <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; \n",
       "                                    border-radius: 10px; border-left: 3px solid #2196F3;\">\n",
       "                            <strong>ü§ñ gpt-4o:</strong><br>\n",
       "                            <div style=\"margin-top: 8px; white-space: pre-wrap;\">This document is a demonstration of an LLM (Large Language Model) Kernel used within a notebook context, showcasing various functionalities. It outlines how to perform basic queries, manage models, generate and optimize code, compare model outputs, and manage context. Additionally, it includes information on pinning important cells, interactive configuration using chat commands, and executing tasks like HTTP requests with given configurations. The primary focus is to illustrate how to effectively utilize the LLM Kernel's features for different tasks in an interactive notebook environment.</div>\n",
       "                        </div>\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "What is the main thesis or argument of this document? Please provide a brief summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                        <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; \n",
       "                                    border-radius: 10px; border-left: 3px solid #2196F3;\">\n",
       "                            <strong>ü§ñ gpt-4o:</strong><br>\n",
       "                            <div style=\"margin-top: 8px; white-space: pre-wrap;\">To extract and list the main section headings from a PDF, you can use a combination of Python libraries such as PyPDF2 or pdfminer to read the PDF content, and then apply regular expressions or string processing to identify and list the headings. Here's a simple example using PyPDF2:\n",
       "\n",
       "```python\n",
       "import PyPDF2\n",
       "\n",
       "def extract_headings_from_pdf(pdf_path):\n",
       "    headings = []\n",
       "    with open(pdf_path, 'rb') as file:\n",
       "        reader = PyPDF2.PdfReader(file)\n",
       "        for page in reader.pages:\n",
       "            text = page.extract_text()\n",
       "            # Simple logic to extract headings might include lines with all caps or specific formatting\n",
       "            # Below is a basic example of capturing lines that might represent headings\n",
       "            for line in text.split('\\n'):\n",
       "                if line.isupper():  # Assuming headings are in all caps\n",
       "                    headings.append(line)\n",
       "    return headings\n",
       "\n",
       "pdf_path = 'your_document.pdf'  # Replace with your PDF file path\n",
       "headings = extract_headings_from_pdf(pdf_path)\n",
       "print('Headings found in PDF:')\n",
       "for heading in headings:\n",
       "    print(heading)\n",
       "```\n",
       "\n",
       "Remember to replace `'your_document.pdf'` with the actual path to your PDF file. This script assumes that headings in the PDF are formatted in uppercase. Depending on the actual formatting of your PDF, you might need to adjust the logic used for identifying headings.</div>\n",
       "                        </div>\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Can you extract and list the main section headings from this PDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Removed 1 file uploads from conversation history\n"
     ]
    }
   ],
   "source": [
    "# Clear files before testing next provider\n",
    "%llm_files_clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Claude (3.5 Sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Model 'claude-3-5-sonnet' not available\n",
      "Available models: gpt-4o, gpt-4o-mini, gpt-4, gpt-3.5-turbo, claude-3-opus, claude-3-sonnet, claude-3-haiku, ollama/llama3, ollama/codellama, ollama/mistral\n"
     ]
    }
   ],
   "source": [
    "# Switch to Claude\n",
    "%llm_model claude-3-5-sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded PDF 'the_measure_of_apocalypse.pdf' to OpenAI (file_id: file-RVdkEWKuWzULAbZMhyeqgU)\n",
      "üí° You can now ask questions about this PDF in any cell\n"
     ]
    }
   ],
   "source": [
    "# Upload the same PDF\n",
    "%llm_pdf_native {TEST_PDF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                        <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; \n",
       "                                    border-radius: 10px; border-left: 3px solid #2196F3;\">\n",
       "                            <strong>ü§ñ gpt-4o:</strong><br>\n",
       "                            <div style=\"margin-top: 8px; white-space: pre-wrap;\">The document is a notebook demonstrating the features of an LLM (Large Language Model) Kernel. It is structured into multiple sections, each showcasing different functionalities of the LLM Kernel: basic LLM queries, model management, code generation with context, model comparison, context management, and interactive configuration. Briefly, it shows how to execute queries, manage models, generate optimized code, pin cells for context, and configure settings interactively.</div>\n",
       "                        </div>\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "What is the main thesis or argument of this document? Please provide a brief summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                        <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; \n",
       "                                    border-radius: 10px; border-left: 3px solid #2196F3;\">\n",
       "                            <strong>ü§ñ gpt-4o:</strong><br>\n",
       "                            <div style=\"margin-top: 8px; white-space: pre-wrap;\">The document you've provided doesn't contain any explicit mathematical formulas or equations. It primarily consists of markdown instructions and Python code snippets related to using a notebook interface with language model features, such as making API requests, defining functions, and utilizing the context for code execution.\n",
       "\n",
       "If you have specific parts of another document containing mathematical content you would like help with, please provide those details, and I'd be happy to assist!</div>\n",
       "                        </div>\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Can you identify any mathematical formulas or equations in this document and explain what they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Removed 1 file uploads from conversation history\n"
     ]
    }
   ],
   "source": [
    "# Clear files\n",
    "%llm_files_clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Gemini (2.0 Flash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Model 'gemini-2.0-flash' not available\n",
      "Available models: gpt-4o, gpt-4o-mini, gpt-4, gpt-3.5-turbo, claude-3-opus, claude-3-sonnet, claude-3-haiku, ollama/llama3, ollama/codellama, ollama/mistral\n"
     ]
    }
   ],
   "source": [
    "# Switch to Gemini\n",
    "%llm_model gemini-2.0-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded PDF 'the_measure_of_apocalypse.pdf' to OpenAI (file_id: file-RVdkEWKuWzULAbZMhyeqgU)\n",
      "üí° You can now ask questions about this PDF in any cell\n"
     ]
    }
   ],
   "source": [
    "# Upload the PDF to Gemini\n",
    "%llm_pdf_native {TEST_PDF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is the main thesis or argument of this document? Please provide a brief summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "How many pages does this document have, and what topics are covered in the conclusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test File Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all uploaded files\n",
    "%llm_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cache info\n",
    "%llm_cache_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List cached files\n",
    "%llm_cache_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear everything\n",
    "%llm_files_clear\n",
    "%llm_clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This test verifies that:\n",
    "1. **OpenAI** uses the Assistants API for true PDF reading\n",
    "2. **Claude** uses the Files API (beta) for native PDF support\n",
    "3. **Gemini** uses Google's Files API with efficient token usage\n",
    "\n",
    "All three providers can:\n",
    "- Upload PDFs natively without conversion\n",
    "- Extract and analyze text content\n",
    "- Answer questions about the document\n",
    "- Maintain context across cells"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
