{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6be73d-5cd5-4f26-bbef-3ed88d308173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] DEBUG: Using selector: SelectSelector\n",
      "[LLM Kernel] DEBUG: \n",
      "\n",
      "[LLM Kernel] DEBUG: \u001b[92mRequest to litellm:\u001b[0m\n",
      "[LLM Kernel] DEBUG: \u001b[92mlitellm.completion(model='gpt-4o-mini', messages=[{'role': 'user', 'content': 'Can you hear me?\\n'}])\u001b[0m\n",
      "[LLM Kernel] DEBUG: \n",
      "\n",
      "[LLM Kernel] DEBUG: self.optional_params: {}\n",
      "[LLM Kernel] DEBUG: SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] DEBUG: \n",
      "LiteLLM: Params passed to completion() {'model': 'gpt-4o-mini', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Can you hear me?\\n'}], 'thinking': None, 'web_search_options': None}\n",
      "[LLM Kernel] DEBUG: \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "[LLM Kernel] DEBUG: Final returned optional params: {'extra_body': {}}\n",
      "[LLM Kernel] DEBUG: self.optional_params: {'extra_body': {}}\n",
      "[LLM Kernel] DEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-mini', 'combined_model_name': 'openai/gpt-4o-mini', 'stripped_model_name': 'gpt-4o-mini', 'combined_stripped_model_name': 'openai/gpt-4o-mini', 'custom_llm_provider': 'openai'}\n",
      "[LLM Kernel] DEBUG: \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.openai.com/v1/ \\\n",
      "-d '{'model': 'gpt-4o-mini', 'messages': [{'role': 'user', 'content': 'Can you hear me?\\n'}], 'extra_body': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "[LLM Kernel] DEBUG: Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Raw-Response': 'true'}, 'timeout': 600.0, 'files': None, 'idempotency_key': 'stainless-python-retry-eef844db-8407-45ee-b443-19700d97e529', 'json_data': {'messages': [{'role': 'user', 'content': 'Can you hear me?\\n'}], 'model': 'gpt-4o-mini'}, 'extra_json': {}}\n",
      "[LLM Kernel] DEBUG: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "[LLM Kernel] DEBUG: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "[LLM Kernel] DEBUG: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002223A30A0D0>\n",
      "[LLM Kernel] DEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022238FB5B50> server_hostname='api.openai.com' timeout=600.0\n",
      "[LLM Kernel] DEBUG: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002223906A3F0>\n",
      "[LLM Kernel] DEBUG: send_request_headers.started request=<Request [b'POST']>\n",
      "[LLM Kernel] DEBUG: send_request_headers.complete\n",
      "[LLM Kernel] DEBUG: send_request_body.started request=<Request [b'POST']>\n",
      "[LLM Kernel] DEBUG: send_request_body.complete\n",
      "[LLM Kernel] DEBUG: receive_response_headers.started request=<Request [b'POST']>\n",
      "[LLM Kernel] DEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 04 Jul 2025 04:29:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-e1ijymzf1s1a1bikzu2oab47'), (b'openai-processing-ms', b'564'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'571'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199992'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'req_9cd54170a65d925645bc6251f5b5be11'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=rLl2nXjDTx4NqLbBsE49DyeYoR61MgVrCQspwsQItf4-1751603341-1.0.1.1-vjtwT94JFvzHMqf1CMwf4prq9hZZ5sYfdbKBsVp2I1h8xBC7wz6P55QlLUVbcOPN0T3kWOan1cembVPNMPOHOAE52cYKMVCmoEaZsPUs0mA; path=/; expires=Fri, 04-Jul-25 04:59:01 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=oGKKNxrKRGSOkWazcPkXaeC0FLYX8gxzXg1mECBUO.I-1751603341737-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'959be10e8ce0fa52-SJC'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] DEBUG: receive_response_body.started request=<Request [b'POST']>\n",
      "[LLM Kernel] DEBUG: receive_response_body.complete\n",
      "[LLM Kernel] DEBUG: response_closed.started\n",
      "[LLM Kernel] DEBUG: response_closed.complete\n",
      "[LLM Kernel] DEBUG: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Fri, 04 Jul 2025 04:29:01 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-e1ijymzf1s1a1bikzu2oab47'), ('openai-processing-ms', '564'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '571'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '199992'), ('x-ratelimit-reset-requests', '8.64s'), ('x-ratelimit-reset-tokens', '2ms'), ('x-request-id', 'req_9cd54170a65d925645bc6251f5b5be11'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=rLl2nXjDTx4NqLbBsE49DyeYoR61MgVrCQspwsQItf4-1751603341-1.0.1.1-vjtwT94JFvzHMqf1CMwf4prq9hZZ5sYfdbKBsVp2I1h8xBC7wz6P55QlLUVbcOPN0T3kWOan1cembVPNMPOHOAE52cYKMVCmoEaZsPUs0mA; path=/; expires=Fri, 04-Jul-25 04:59:01 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=oGKKNxrKRGSOkWazcPkXaeC0FLYX8gxzXg1mECBUO.I-1751603341737-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '959be10e8ce0fa52-SJC'), ('content-encoding', 'br'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "[LLM Kernel] DEBUG: request_id: req_9cd54170a65d925645bc6251f5b5be11\n",
      "[LLM Kernel] DEBUG: RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-BpSKjO1M8kwTuUXDIc8vWoWmVjKN7\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"I can \\\"read\\\" your messages, but I don't have the capability to hear. How can I assist you today?\", \"refusal\": null, \"role\": \"assistant\", \"annotations\": [], \"audio\": null, \"function_call\": null, \"tool_calls\": null}}], \"created\": 1751603341, \"model\": \"gpt-4o-mini-2024-07-18\", \"object\": \"chat.completion\", \"service_tier\": \"default\", \"system_fingerprint\": \"fp_62a23a81ef\", \"usage\": {\"completion_tokens\": 24, \"prompt_tokens\": 12, \"total_tokens\": 36, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": 0, \"reasoning_tokens\": 0, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": 0, \"cached_tokens\": 0}}}\n",
      "\n",
      "\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n",
      "[LLM Kernel] DEBUG: Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "[LLM Kernel] DEBUG: selected model name for cost calculation: openai/gpt-4o-mini-2024-07-18\n",
      "[LLM Kernel] DEBUG: selected model name for cost calculation: openai/gpt-4o-mini-2024-07-18\n",
      "[LLM Kernel] DEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-mini-2024-07-18', 'combined_model_name': 'openai/gpt-4o-mini-2024-07-18', 'stripped_model_name': 'gpt-4o-mini-2024-07-18', 'combined_stripped_model_name': 'openai/gpt-4o-mini-2024-07-18', 'custom_llm_provider': 'openai'}\n",
      "[LLM Kernel] DEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-mini-2024-07-18', 'combined_model_name': 'openai/gpt-4o-mini-2024-07-18', 'stripped_model_name': 'gpt-4o-mini-2024-07-18', 'combined_stripped_model_name': 'openai/gpt-4o-mini-2024-07-18', 'custom_llm_provider': 'openai'}\n",
      "[LLM Kernel] DEBUG: model_info: {'key': 'gpt-4o-mini-2024-07-18', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 1.5e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': 7.5e-08, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 7.5e-08, 'output_cost_per_token_batches': 3e-07, 'output_cost_per_token': 6e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': {'search_context_size_low': 30.0, 'search_context_size_medium': 35.0, 'search_context_size_high': 50.0}, 'tpm': None, 'rpm': None}\n",
      "[LLM Kernel] DEBUG: model_info: {'key': 'gpt-4o-mini-2024-07-18', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 1.5e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': 7.5e-08, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 7.5e-08, 'output_cost_per_token_batches': 3e-07, 'output_cost_per_token': 6e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': {'search_context_size_low': 30.0, 'search_context_size_medium': 35.0, 'search_context_size_high': 50.0}, 'tpm': None, 'rpm': None}\n",
      "[LLM Kernel] DEBUG: response_cost: 1.62e-05\n",
      "[LLM Kernel] DEBUG: response_cost: 1.62e-05\n",
      "[LLM Kernel] DEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-mini-2024-07-18', 'combined_model_name': 'openai/gpt-4o-mini-2024-07-18', 'stripped_model_name': 'gpt-4o-mini-2024-07-18', 'combined_stripped_model_name': 'openai/gpt-4o-mini-2024-07-18', 'custom_llm_provider': 'openai'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can \"read\" your messages, but I don't have the capability to hear. How can I assist you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] DEBUG: model_info: {'key': 'gpt-4o-mini-2024-07-18', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 1.5e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': 7.5e-08, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 7.5e-08, 'output_cost_per_token_batches': 3e-07, 'output_cost_per_token': 6e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': {'search_context_size_low': 30.0, 'search_context_size_medium': 35.0, 'search_context_size_high': 50.0}, 'tpm': None, 'rpm': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I can \"read\" your messages, but I don\\'t have the capability to hear. How can I assist you today?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] DEBUG: Logging Details LiteLLM-Success Call streaming complete\n",
      "[LLM Kernel] DEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o-mini-2024-07-18', 'combined_model_name': 'openai/gpt-4o-mini-2024-07-18', 'stripped_model_name': 'gpt-4o-mini-2024-07-18', 'combined_stripped_model_name': 'openai/gpt-4o-mini-2024-07-18', 'custom_llm_provider': 'openai'}\n",
      "[LLM Kernel] DEBUG: model_info: {'key': 'gpt-4o-mini-2024-07-18', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 1.5e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': 7.5e-08, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 7.5e-08, 'output_cost_per_token_batches': 3e-07, 'output_cost_per_token': 6e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'citation_cost_per_token': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': True, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': True, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': {'search_context_size_low': 30.0, 'search_context_size_medium': 35.0, 'search_context_size_high': 50.0}, 'tpm': None, 'rpm': None}\n"
     ]
    }
   ],
   "source": [
    "%%llm\n",
    "Can you hear me?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662e9be-7a53-4c1a-bc0a-7bebfe9bdfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
