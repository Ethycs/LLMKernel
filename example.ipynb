{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Kernel Example\n",
    "\n",
    "This notebook demonstrates the key features of the LLM Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic LLM Queries\n",
    "\n",
    "Use the `%%llm` magic to query the active model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n",
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n",
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n",
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n"
     ]
    }
   ],
   "source": [
    "%%llm\n",
    "What is the capital of France? Give a brief answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Management\n",
    "\n",
    "List available models and switch between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%llm_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Generation with Context\n",
    "\n",
    "The LLM can see and understand previous cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple function\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%llm\n",
    "Can you optimize the fibonacci function I defined above using memoization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison\n",
    "\n",
    "Compare responses from multiple models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%llm_compare gpt-4o-mini claude-3-haiku\n",
    "Explain recursion in one sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Context Management\n",
    "\n",
    "Pin important cells and manage context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important configuration\n",
    "API_ENDPOINT = \"https://api.example.com\"\n",
    "MAX_RETRIES = 3\n",
    "TIMEOUT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pin the configuration cell (replace 5 with actual cell number)\n",
    "%llm_pin_cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%llm\n",
    "Write a Python function that uses the API configuration I defined to make a GET request with retry logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Configuration\n",
    "\n",
    "Use the config widget to adjust settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afaaa6cdc364dcc91f22e3e131d049e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>ü§ñ LLM Kernel Configuration</h3>'), Dropdown(description='Active Model:', index=‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3792048541.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mThe lucky number is now 7\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%llm_chat\n",
    "\n",
    "The lucky number is now 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%llm_chat\n",
    "The lucky number is now 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: ON\n",
      "üìì Notebook context mode: ON\n",
      "üìù Just type in any cell to chat!\n",
      "üí° Your notebook cells are now the LLM's context window!\n"
     ]
    }
   ],
   "source": [
    "%llm_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "Got it! The lucky number is now 5.\n",
      "\n",
      "========================================\n",
      "üí¨ Continue in next cell with %%llm\n"
     ]
    }
   ],
   "source": [
    "The lucky number is now 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "Got it! The lucky number is now 6.\n",
      "\n",
      "========================================\n",
      "üí¨ Continue in next cell with %%llm\n"
     ]
    }
   ],
   "source": [
    "The lucky number is now 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "The lucky number is 7.\n",
      "\n",
      "========================================\n",
      "üí¨ Continue in next cell with %%llm\n"
     ]
    }
   ],
   "source": [
    "What is the lucky number?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
