{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Reranking & Custom Meta Functions Demo\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. LLM-based context reranking by relevance\n",
    "2. Custom meta functions for filtering, ranking, and transforming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: ON\n",
      "üìì Notebook context mode: ON\n",
      "üìù Just type in any cell to chat!\n",
      "üí° Your notebook cells are now the LLM's context window!\n"
     ]
    }
   ],
   "source": [
    "# Enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build Some Context\n",
    "\n",
    "Let's create a variety of cells with different topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell about Python basics\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "    \n",
    "hello_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell about machine learning\n",
    "import numpy as np\n",
    "\n",
    "# Simple linear regression example\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = 2 * X + 1\n",
    "print(f\"Linear relationship: y = 2x + 1\")\n",
    "print(f\"X: {X}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                        <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; \n",
       "                                    border-radius: 10px; border-left: 3px solid #2196F3;\">\n",
       "                            <strong>ü§ñ gpt-4o:</strong><br>\n",
       "                            <div style=\"margin-top: 8px; white-space: pre-wrap;\">Gradient descent is an optimization algorithm commonly used to minimize the cost function in machine learning and deep learning. It is an iterative method that helps in finding the local minimum of a differentiable function. \n",
       "\n",
       "Here's how gradient descent works:\n",
       "\n",
       "1. **Initialization**: Start with an initial guess for the model parameters (weights). This can be random or set to zero.\n",
       "\n",
       "2. **Compute the Cost**: Calculate the value of the cost function using the current parameters. The cost function measures how well the model's predictions align with the actual data.\n",
       "\n",
       "3. **Compute the Gradient**: Calculate the gradient of the cost function with respect to each parameter. The gradient is a vector of partial derivatives, indicating the direction and rate of change of the cost for each parameter.\n",
       "\n",
       "4. **Update the Parameters**: Adjust the parameters in the opposite direction of the gradient. This step moves the parameters towards the minimum of the cost function. The update rule is typically:\n",
       "   \\[\n",
       "   \\theta := \\theta - \\alpha \\cdot \\nabla J(\\theta)\n",
       "   \\]\n",
       "   where \\( \\theta \\) represents the parameters, \\( \\alpha \\) is the learning rate, and \\( \\nabla J(\\theta) \\) is the gradient of the cost function.\n",
       "\n",
       "5. **Repeat**: Repeat the process for a predetermined number of iterations or until the change in the cost function is below a certain threshold.\n",
       "\n",
       "6. **Convergence Check**: Optionally, monitor if the algorithm has converged, meaning further updates result in negligible changes in the cost function.\n",
       "\n",
       "The learning rate (\\( \\alpha \\)) is a crucial hyperparameter that determines the size of the steps taken towards the minimum. A learning rate that is too small can result in a slow convergence, while too large a learning rate might cause the algorithm to overshoot the minimum or diverge.\n",
       "\n",
       "There are several variants of gradient descent, including:\n",
       "\n",
       "- **Batch Gradient Descent**: Uses the entire dataset to compute the gradient at each iteration.\n",
       "- **Stochastic Gradient Descent (SGD)**: Uses only one randomly selected example from the dataset for computing the gradient for each iteration, which can lead to faster, though noisier, convergence.\n",
       "- **Mini-batch Gradient Descent**: A compromise between batch and stochastic gradient descent, using a small, random subset of the dataset to compute the gradient.\n",
       "\n",
       "These variants help in dealing with different challenges like computational efficiency and convergence dynamics.</div>\n",
       "                        </div>\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "What is gradient descent and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell about web development\n",
    "html_template = \"\"\"\n",
    "<html>\n",
    "  <head><title>My Page</title></head>\n",
    "  <body>\n",
    "    <h1>Welcome!</h1>\n",
    "    <p>This is a simple HTML page.</p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "print(\"Basic HTML structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random cell about cooking\n",
    "recipe = {\n",
    "    \"name\": \"Chocolate Chip Cookies\",\n",
    "    \"ingredients\": [\"flour\", \"butter\", \"sugar\", \"eggs\", \"chocolate chips\"],\n",
    "    \"time\": \"30 minutes\"\n",
    "}\n",
    "print(f\"Recipe: {recipe['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                        <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; \n",
       "                                    border-radius: 10px; border-left: 3px solid #2196F3;\">\n",
       "                            <strong>ü§ñ gpt-4o:</strong><br>\n",
       "                            <div style=\"margin-top: 8px; white-space: pre-wrap;\">Certainly! Neural networks are a subset of machine learning models inspired by the human brain's structure and function. They consist of layers of interconnected nodes, or \"neurons,\" that work together to process and learn from data. Here's a simplified breakdown of how they work:\n",
       "\n",
       "1. **Layers of Neurons**: \n",
       "   - A neural network typically comprises an input layer, one or more hidden layers, and an output layer.\n",
       "   - Each layer consists of neurons, which are units that take data inputs, perform computations, and pass their outputs to the next layer.\n",
       "\n",
       "2. **Connections and Weights**: \n",
       "   - Neurons in one layer are connected to neurons in the next layer through weighted connections.\n",
       "   - Weights represent the strength or importance of a connection, and adjusting these weights is how the network learns from data.\n",
       "\n",
       "3. **Activation Functions**: \n",
       "   - After receiving input, each neuron applies an activation function to determine its output. \n",
       "   - Activation functions add non-linearity to the network, enabling it to learn complex relationships in the data. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n",
       "\n",
       "4. **Training**: \n",
       "   - The process of training a neural network involves adjusting the weights of the connections based on the error between the network's predictions and the actual data.\n",
       "   - This is typically done using a technique called backpropagation, combined with an optimization algorithm like gradient descent.\n",
       "\n",
       "5. **Learning**: \n",
       "   - During training, the network learns patterns in the input data. It gradually improves its predictions by minimizing the error through multiple iterations over the data, which are called epochs.\n",
       "\n",
       "6. **Applications**: \n",
       "   - Neural networks are used in various applications, such as image and speech recognition, language processing, and even playing games.\n",
       "\n",
       "Neural networks have a great capacity for learning complex patterns and are the foundation for deep learning, where networks have many hidden layers (hence \"deep\") to capture intricate patterns in data.</div>\n",
       "                        </div>\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Can you explain neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: LLM-Based Reranking\n",
    "\n",
    "Now let's rerank cells by relevance to a specific query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìì Notebook Context Mode - Showing cells that will be sent to LLM:\n",
      "============================================================\n",
      "\n",
      "[1] USER:\n",
      "# Enable chat mode\n",
      "%llm_chat\n",
      "----------------------------------------\n",
      "\n",
      "[2] USER:\n",
      "# Enable chat mode\n",
      "%llm_chat on\n",
      "----------------------------------------\n",
      "\n",
      "[3] USER:\n",
      "# First, let's see the current context order\n",
      "%llm_context\n",
      "----------------------------------------\n",
      "\n",
      "Total messages: 3\n",
      "Token usage: 48 / 16,384 (0.3%)\n",
      "[‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]\n"
     ]
    }
   ],
   "source": [
    "# First, let's see the current context order\n",
    "%llm_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Reranking cells based on: '\"machine learning and neural networks\"'\n",
      "‚è≥ Asking LLM to analyze relevance...\n",
      "‚úÖ Reranked 4 cells by relevance\n",
      "üìù Context has been reordered (most relevant first)\n",
      "üí° Use %llm_context to see the new order\n"
     ]
    }
   ],
   "source": [
    "# Rerank by relevance to machine learning\n",
    "%llm_rerank \"machine learning and neural networks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìì Notebook Context Mode - Showing cells that will be sent to LLM:\n",
      "============================================================\n",
      "\n",
      "[1] USER:\n",
      "# Rerank by relevance to machine learning\n",
      "%llm_rerank \"machine learning and neural networks\"\n",
      "----------------------------------------\n",
      "\n",
      "[2] USER:\n",
      "# First, let's see the current context order\n",
      "%llm_context\n",
      "----------------------------------------\n",
      "\n",
      "[3] USER:\n",
      "# Enable chat mode\n",
      "%llm_chat\n",
      "----------------------------------------\n",
      "\n",
      "[4] USER:\n",
      "# Enable chat mode\n",
      "%llm_chat on\n",
      "----------------------------------------\n",
      "\n",
      "[5] USER:\n",
      "Analyze the following conversation cells and rank them by relevance to this query:\n",
      "\"\"machine learning and neural networks\"\"\n",
      "\n",
      "Here are the cells to rank:\n",
      "\n",
      "[Cell 1] USER:\n",
      "# Rerank by relevance to machin...\n",
      "----------------------------------------\n",
      "\n",
      "Total messages: 5\n",
      "Token usage: 249 / 16,384 (1.5%)\n",
      "[‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]\n"
     ]
    }
   ],
   "source": [
    "# Check the new order\n",
    "%llm_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Reranking cells based on: '\"web development\"'\n",
      "‚è≥ Asking LLM to analyze relevance...\n",
      "\n",
      "üìä Relevance Ranking:\n",
      "==================================================\n",
      "1. [Cell 2] USER: # First, let's see the current context order\n",
      "%llm_context\n",
      "2. [Cell 3] USER: # Enable chat mode\n",
      "%llm_chat\n",
      "3. [Cell 4] USER: # Enable chat mode\n",
      "%llm_chat on\n",
      "4. [Cell 1] USER: # Rerank by relevance to machine learning\n",
      "%llm_rerank \"machine learning and neur...\n",
      "5. [Cell 5] USER: Analyze the following conversation cells and rank them by relevance to this quer...\n"
     ]
    }
   ],
   "source": [
    "# Just show ranking without reordering\n",
    "%llm_rerank --show \"web development\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Reranking cells based on: '\"Python programming\"'\n",
      "‚è≥ Asking LLM to analyze relevance...\n",
      "üìä Keeping top 5 most relevant cells\n",
      "‚úÖ Reranked 5 cells by relevance\n",
      "üìù Context has been reordered (most relevant first)\n",
      "üí° Use %llm_context to see the new order\n"
     ]
    }
   ],
   "source": [
    "# Keep only top 5 most relevant cells\n",
    "%llm_rerank --top=5 \"Python programming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleared reranking - context restored to original order\n"
     ]
    }
   ],
   "source": [
    "# Clear reranking to restore original order\n",
    "%llm_rerank_clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Custom Meta Functions\n",
    "\n",
    "Define your own context processing logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%meta filter\n",
    "def filter_cells(messages):\n",
    "    \"\"\"Filter out cells that are too short or contain specific keywords.\"\"\"\n",
    "    filtered = []\n",
    "    \n",
    "    for msg in messages:\n",
    "        content = msg['content']\n",
    "        \n",
    "        # Skip very short messages\n",
    "        if len(content) < 20:\n",
    "            continue\n",
    "            \n",
    "        # Skip cells about cooking (as an example)\n",
    "        if 'recipe' in content.lower() or 'cooking' in content.lower():\n",
    "            continue\n",
    "            \n",
    "        filtered.append(msg)\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%meta ranking\n",
    "def rank_cells(messages, query):\n",
    "    \"\"\"Custom ranking based on keyword matching.\"\"\"\n",
    "    # Simple keyword-based ranking\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    scored_messages = []\n",
    "    for msg in messages:\n",
    "        content_words = set(msg['content'].lower().split())\n",
    "        score = len(query_words.intersection(content_words))\n",
    "        scored_messages.append((score, msg))\n",
    "    \n",
    "    # Sort by score (descending)\n",
    "    scored_messages.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Return reordered messages\n",
    "    return [msg for score, msg in scored_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%meta transform\n",
    "def transform_context(messages):\n",
    "    \"\"\"Add metadata to each message.\"\"\"\n",
    "    transformed = []\n",
    "    \n",
    "    for i, msg in enumerate(messages):\n",
    "        # Add position and length metadata\n",
    "        new_msg = msg.copy()\n",
    "        new_msg['content'] = f\"[Position {i+1}, Length {len(msg['content'])}]\\n{msg['content']}\"\n",
    "        transformed.append(new_msg)\n",
    "    \n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List defined meta functions\n",
    "%llm_meta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter function\n",
    "%llm_apply_meta filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply custom ranking\n",
    "%llm_apply_meta ranking \"machine learning neural networks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all meta functions in sequence\n",
    "%llm_apply_meta all \"Python programming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the transformed context\n",
    "%llm_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Meta Functions\n",
    "\n",
    "More sophisticated examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%meta ranking\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def rank_cells(messages, query):\n",
    "    \"\"\"Semantic ranking using TF-IDF-like scoring.\"\"\"\n",
    "    # Tokenize query\n",
    "    query_tokens = re.findall(r'\\w+', query.lower())\n",
    "    query_freq = Counter(query_tokens)\n",
    "    \n",
    "    scored = []\n",
    "    for msg in messages:\n",
    "        # Tokenize message\n",
    "        msg_tokens = re.findall(r'\\w+', msg['content'].lower())\n",
    "        msg_freq = Counter(msg_tokens)\n",
    "        \n",
    "        # Calculate similarity score\n",
    "        score = 0\n",
    "        for token, freq in query_freq.items():\n",
    "            if token in msg_freq:\n",
    "                # Higher score for more occurrences\n",
    "                score += min(freq, msg_freq[token]) * (1 + len(token) / 10)\n",
    "        \n",
    "        # Boost score for assistant responses about the topic\n",
    "        if msg['role'] == 'assistant' and any(t in msg_tokens for t in query_tokens):\n",
    "            score *= 1.5\n",
    "            \n",
    "        scored.append((score, msg))\n",
    "    \n",
    "    # Sort by score\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Return reordered, keeping at least some context\n",
    "    result = [msg for _, msg in scored]\n",
    "    \n",
    "    # Ensure we have at least 3 messages\n",
    "    if len(result) < 3 and len(messages) >= 3:\n",
    "        return result + messages[:3-len(result)]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the new ranking\n",
    "%llm_apply_meta ranking \"neural networks gradient descent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You now have powerful tools for context management:\n",
    "\n",
    "1. **LLM-based reranking** - Let the AI intelligently reorder cells by relevance\n",
    "2. **Custom meta functions** - Define your own logic for filtering, ranking, and transforming\n",
    "3. **Flexible application** - Apply functions individually or in sequence\n",
    "\n",
    "This gives you complete control over what context the LLM sees and in what order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable chat mode\n",
    "%llm_chat off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
