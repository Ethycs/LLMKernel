{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Kernel - Chat Mode & Notebook Context Demo\n",
    "\n",
    "This notebook demonstrates the key features of the LLM Kernel, including:\n",
    "- Chat mode for natural conversation\n",
    "- Notebook cells as literal context window\n",
    "- Various LLM integrations and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Setup\n",
    "\n",
    "First, let's check our available models and kernel status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Available LLM Models:\n",
      "  - gpt-4o: gpt-4o ‚úÖ (active)\n",
      "  - gpt-4o-mini: gpt-4o-mini\n",
      "  - gpt-4: gpt-4\n",
      "  - gpt-3.5-turbo: gpt-3.5-turbo\n",
      "  - claude-3-opus: claude-3-opus-20240229\n",
      "  - claude-3-sonnet: claude-3-sonnet-20240229\n",
      "  - claude-3-haiku: claude-3-haiku-20240307\n",
      "  - ollama/llama3: ollama/llama3\n",
      "  - ollama/codellama: ollama/codellama\n",
      "  - ollama/mistral: ollama/mistral\n"
     ]
    }
   ],
   "source": [
    "# Check available LLM models\n",
    "%llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ LLM Kernel Status\n",
      "========================================\n",
      "Active Model: gpt-4o\n",
      "Available Models: 10\n",
      "\n",
      "Conversation History: 0 exchanges\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ContextManager' object has no attribute 'get_window_usage'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check current kernel status\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mllm_status\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\Keytone\\Documents\\GitHub\\LLMKernel\\.pixi\\envs\\notebook\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2504\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2502\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2504\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2506\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2507\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2508\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mF:\\Keytone\\Documents\\GitHub\\LLMKernel\\llm_kernel\\magic_commands\\base.py:112\u001b[39m, in \u001b[36mBaseMagics.llm_status\u001b[39m\u001b[34m(self, line)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Context window usage\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.kernel, \u001b[33m'\u001b[39m\u001b[33mcontext_manager\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     window_usage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontext_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_window_usage\u001b[49m()\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mContext Window Usage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_usage\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Token usage estimate\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'ContextManager' object has no attribute 'get_window_usage'"
     ]
    }
   ],
   "source": [
    "# Check current kernel status\n",
    "%llm_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traditional LLM Queries\n",
    "\n",
    "Before we enable chat mode, let's see how traditional queries work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The capital of France is Paris."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%llm\n",
    "What is the capital of France?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Here are three interesting facts about Paris:\n",
       "\n",
       "1. **The Eiffel Tower's Temporary Nature**: The Eiffel Tower, now a globally recognized symbol of Paris, was originally intended to be a temporary structure. It was built by Gustave Eiffel for the 1889 Exposition Universelle (World's Fair) to commemorate the 100th anniversary of the French Revolution. The tower was supposed to be dismantled after 20 years, but its popularity and usefulness as a radiotelegraph station ensured its survival.\n",
       "\n",
       "2. **A Hub for Intellectual and Artistic Movements**: Throughout the 19th and 20th centuries, Paris was a major center for artistic and intellectual movements. It attracted and inspired numerous artists and writers such as Pablo Picasso, Ernest Hemingway, Gertrude Stein, and many others. The city played a pivotal role in the development of movements like Impressionism, Surrealism, and Cubism.\n",
       "\n",
       "3. **The Paris Catacombs**: Beneath the bustling streets of Paris lies a network of tunnels known as the Catacombs. Originally limestone quarries, the tunnels were later used as an ossuary in the late 18th century to house the remains of millions, as Paris's cemeteries became overcrowded. Today, the Catacombs are a popular tourist attraction and an intriguing piece of Paris's underground history."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%llm\n",
    "Now tell me three interesting facts about that city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chat Mode - The Magic Begins! üéâ\n",
    "\n",
    "Now let's enable chat mode. This does two things:\n",
    "1. Allows you to type naturally in cells without `%%llm`\n",
    "2. Automatically enables notebook context mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: ON\n",
      "üìì Notebook context mode: ON\n",
      "üìù Just type in any cell to chat!\n",
      "üí° Your notebook cells are now the LLM's context window!\n"
     ]
    }
   ],
   "source": [
    "# Enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Natural Conversation\n",
    "\n",
    "Now you can just type in cells! No magic commands needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                        <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; \n",
       "                                    border-radius: 10px; border-left: 3px solid #2196F3;\">\n",
       "                            <strong>ü§ñ gpt-4o:</strong><br>\n",
       "                            <div style=\"margin-top: 8px; white-space: pre-wrap;\">Hello! I can see the context of our current interaction, but I don't have access to previous questions or any specific past interactions unless they were part of this ongoing session. If you have any questions about France, feel free to ask, and I'll be happy to help!</div>\n",
       "                        </div>\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Hello! I'm now in chat mode. Can you see my previous questions about France?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                        <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; \n",
       "                                    border-radius: 10px; border-left: 3px solid #2196F3;\">\n",
       "                            <strong>ü§ñ gpt-4o:</strong><br>\n",
       "                            <div style=\"margin-top: 8px; white-space: pre-wrap;\">15 multiplied by 23 equals 345.</div>\n",
       "                        </div>\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Let's do some math. What's 15 * 23?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                        <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; \n",
       "                                    border-radius: 10px; border-left: 3px solid #2196F3;\">\n",
       "                            <strong>ü§ñ gpt-4o:</strong><br>\n",
       "                            <div style=\"margin-top: 8px; white-space: pre-wrap;\">Could you please provide the number you'd like me to add 100 to?</div>\n",
       "                        </div>\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Great! Now add 100 to that result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Notebook Context - Your Cells ARE the Context!\n",
    "\n",
    "Let's see what context the LLM actually sees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìì Notebook Context Mode - Showing cells that will be sent to LLM:\n",
      "============================================================\n",
      "\n",
      "[1] USER:\n",
      "# Check available LLM models\n",
      "%llm_models\n",
      "----------------------------------------\n",
      "\n",
      "[2] USER:\n",
      "# Check current kernel status\n",
      "%llm_status\n",
      "----------------------------------------\n",
      "\n",
      "[3] USER:\n",
      "# Show current context\n",
      "%llm_context\n",
      "----------------------------------------\n",
      "\n",
      "[4] USER:\n",
      "# Enable chat mode\n",
      "%llm_chat on\n",
      "----------------------------------------\n",
      "\n",
      "[5] USER:\n",
      "# Show current context\n",
      "%llm_context\n",
      "----------------------------------------\n",
      "\n",
      "Total messages: 5\n",
      "Token usage: 72 / 16,384 (0.4%)\n",
      "[‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]\n"
     ]
    }
   ],
   "source": [
    "# Show current context\n",
    "%llm_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Code Understanding\n",
    "\n",
    "The LLM can see and understand code in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                        <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; \n",
       "                                    border-radius: 10px; border-left: 3px solid #2196F3;\">\n",
       "                            <strong>ü§ñ gpt-4o:</strong><br>\n",
       "                            <div style=\"margin-top: 8px; white-space: pre-wrap;\">The code you've provided defines a function to calculate Fibonacci numbers using a recursive approach. It then calculates and prints the first 10 Fibonacci numbers. Here is the list of these numbers: \n",
       "\n",
       "```\n",
       "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
       "```\n",
       "\n",
       "Each number in this sequence is the sum of the two preceding numbers, starting from 0 and 1.</div>\n",
       "                        </div>\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "# Calculate first 10 fibonacci numbers\n",
    "fib_numbers = [fibonacci(i) for i in range(10)]\n",
    "print(fib_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                        <div style=\"margin: 10px 0 20px 40px; padding: 10px; background: #f5f5f5; \n",
       "                                    border-radius: 10px; border-left: 3px solid #2196F3;\">\n",
       "                            <strong>ü§ñ gpt-4o:</strong><br>\n",
       "                            <div style=\"margin-top: 8px; white-space: pre-wrap;\">Sure, please share the Fibonacci function you wrote, and I'll explain it as well as suggest any improvements.</div>\n",
       "                        </div>\n",
       "                        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Can you explain the fibonacci function I just wrote? Also, is there a more efficient way to implement it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Model Comparison\n",
    "\n",
    "You can compare responses from different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: OFF\n",
      "üìì Notebook context mode: OFF\n"
     ]
    }
   ],
   "source": [
    "# First, let's turn off chat mode temporarily\n",
    "%llm_chat off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Comparing 2 models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Keytone\\Documents\\GitHub\\LLMKernel\\.pixi\\envs\\notebook\\Lib\\site-packages\\httpx\\_models.py:408: DeprecationWarning: Use 'content=<...>' to upload raw bytes/text content.\n",
      "  headers, stream = encode_request(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display: flex; gap: 20px;\">\n",
       "                <div style=\"flex: 1; border: 1px solid #ddd; padding: 10px; border-radius: 5px;\">\n",
       "                    <h3 style=\"margin-top: 0;\">ü§ñ gpt-4o-mini</h3>\n",
       "                    <div style=\"max-height: 400px; overflow-y: auto;\">\n",
       "                        Lines of code align,  <br>Logic flows like rivers deep,  <br>Dreams in bytes take flight.\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div style=\"flex: 1; border: 1px solid #ddd; padding: 10px; border-radius: 5px;\">\n",
       "                    <h3 style=\"margin-top: 0;\">ü§ñ claude-3-haiku</h3>\n",
       "                    <div style=\"max-height: 400px; overflow-y: auto;\">\n",
       "                        Here is a haiku about programming:<br><br>Lines of code unfurl<br>Solving problems, bit by bit<br>Logic takes flight\n",
       "                    </div>\n",
       "                </div>\n",
       "                </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%llm_compare gpt-4o-mini claude-3-haiku\n",
    "Write a haiku about programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: ON\n",
      "üìì Notebook context mode: ON\n",
      "üìù Just type in any cell to chat!\n",
      "üí° Your notebook cells are now the LLM's context window!\n"
     ]
    }
   ],
   "source": [
    "# Re-enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Context Management\n",
    "\n",
    "You can manage how context is handled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%llm_strategy` not found.\n"
     ]
    }
   ],
   "source": [
    "# Check current context strategy\n",
    "%llm_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìì Notebook context mode: ON\n"
     ]
    }
   ],
   "source": [
    "# You can also manually toggle notebook context mode\n",
    "%llm_notebook_context status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Debugging Support\n",
    "\n",
    "For development, you can enable debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable debugger (for VS Code)\n",
    "# %llm_debug 5678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conversation Continuation\n",
    "\n",
    "The beauty of notebook context is that your entire notebook becomes the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Based on everything we've discussed in this notebook, can you summarize the key features of the LLM Kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Tips and Tricks\n",
    "\n",
    "- In chat mode, regular Python code still works normally\n",
    "- Magic commands (starting with %) are not sent to the LLM\n",
    "- Comments (starting with #) are also not sent to the LLM\n",
    "- Empty cells are ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python comment - won't be sent to LLM\n",
    "x = 42\n",
    "print(f\"The answer is {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What was the value of x in the previous cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Disabling Chat Mode\n",
    "\n",
    "When you're done with chat mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status and disable\n",
    "%llm_chat status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%llm_chat off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The LLM Kernel provides:\n",
    "\n",
    "1. **Chat Mode** - Natural conversation without magic commands\n",
    "2. **Notebook Context** - Your cells ARE the context window\n",
    "3. **Multi-Model Support** - Switch between different LLMs\n",
    "4. **Context Visualization** - See what the LLM sees\n",
    "5. **Smart Filtering** - Ignores magic commands and comments\n",
    "6. **Debugging Support** - VS Code integration\n",
    "\n",
    "The notebook becomes a living conversation where every cell contributes to the context!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
