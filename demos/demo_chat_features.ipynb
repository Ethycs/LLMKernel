{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Kernel - Chat Mode & Notebook Context Demo\n",
    "\n",
    "This notebook demonstrates the key features of the LLM Kernel, including:\n",
    "- Chat mode for natural conversation\n",
    "- Notebook cells as literal context window\n",
    "- Various LLM integrations and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Setup\n",
    "\n",
    "First, let's check our available models and kernel status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Available LLM Models:\n",
      "  ‚ö™ gpt-4o\n",
      "  ‚úÖ (active) gpt-4o-mini\n",
      "  ‚ö™ gpt-4\n",
      "  ‚ö™ gpt-3.5-turbo\n",
      "  ‚ö™ claude-3-opus\n",
      "  ‚ö™ claude-3-sonnet\n",
      "  ‚ö™ claude-3-haiku\n",
      "  ‚ö™ llama3\n",
      "  ‚ö™ codellama\n"
     ]
    }
   ],
   "source": [
    "# Check available LLM models\n",
    "%llm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ LLM Kernel Status\n",
      "==================================================\n",
      "Active Model: gpt-4o-mini\n",
      "Available Models: 9\n",
      "Conversation History: 0 exchanges\n"
     ]
    }
   ],
   "source": [
    "# Check current kernel status\n",
    "%llm_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traditional LLM Queries\n",
    "\n",
    "Before we enable chat mode, let's see how traditional queries work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n"
     ]
    }
   ],
   "source": [
    "%%llm\n",
    "What is the capital of France?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are three interesting facts about Paris:\n",
      "\n",
      "1. **The Eiffel Tower's Height**: When it was completed in 1889, the Eiffel Tower stood at 300 meters (984 feet) tall, making it the tallest man-made structure in the world at that time. Today, including its antennas, it reaches a height of around 330 meters (1,083 feet).\n",
      "\n",
      "2. **The Catacombs of Paris**: Beneath the streets of Paris lies an extensive network of tunnels that house the remains of approximately six million people. Originally created as limestone quarries, the Catacombs of Paris were officially opened as an ossuary in the late 18th century to address overcrowding in cemeteries.\n",
      "\n",
      "3. **World's Most Visited City**: Paris is one of the most popular tourist destinations in the world, attracting millions of visitors each year. Famous for its museums, landmarks, and culinary delights, it is estimated that around 30 million tourists visit the city annually. The Louvre Museum, home to the Mona Lisa, is the most visited art museum in the world.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure! Here are three interesting facts about Paris:\\n\\n1. **The Eiffel Tower's Height**: When it was completed in 1889, the Eiffel Tower stood at 300 meters (984 feet) tall, making it the tallest man-made structure in the world at that time. Today, including its antennas, it reaches a height of around 330 meters (1,083 feet).\\n\\n2. **The Catacombs of Paris**: Beneath the streets of Paris lies an extensive network of tunnels that house the remains of approximately six million people. Originally created as limestone quarries, the Catacombs of Paris were officially opened as an ossuary in the late 18th century to address overcrowding in cemeteries.\\n\\n3. **World's Most Visited City**: Paris is one of the most popular tourist destinations in the world, attracting millions of visitors each year. Famous for its museums, landmarks, and culinary delights, it is estimated that around 30 million tourists visit the city annually. The Louvre Museum, home to the Mona Lisa, is the most visited art museum in the world.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%llm\n",
    "Now tell me three interesting facts about that city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chat Mode - The Magic Begins! üéâ\n",
    "\n",
    "Now let's enable chat mode. This does two things:\n",
    "1. Allows you to type naturally in cells without `%%llm`\n",
    "2. Automatically enables notebook context mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Natural Conversation\n",
    "\n",
    "Now you can just type in cells! No magic commands needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hello! I'm now in chat mode. Can you see my previous questions about France?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's do some math. What's 15 * 23?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Great! Now add 100 to that result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Notebook Context - Your Cells ARE the Context!\n",
    "\n",
    "Let's see what context the LLM actually sees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current context\n",
    "%llm_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Code Understanding\n",
    "\n",
    "The LLM can see and understand code in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "# Calculate first 10 fibonacci numbers\n",
    "fib_numbers = [fibonacci(i) for i in range(10)]\n",
    "print(fib_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Can you explain the fibonacci function I just wrote? Also, is there a more efficient way to implement it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Model Comparison\n",
    "\n",
    "You can compare responses from different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's turn off chat mode temporarily\n",
    "%llm_chat off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= claude-3-haiku-20240307; provider = anthropic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Querying 2 models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1fee1a3baf4a46a3b0536b8b335ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='\\n            <div style=\"padding: 15px;\">\\n                <h4>GPT-4O-MINI</h4>\\n  ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%llm_compare gpt-4o-mini claude-3-haiku\n",
    "Write a haiku about programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: ON\n",
      "üìì Notebook context mode: ON\n",
      "üìù Just type in any cell to chat!\n",
      "üí° Your notebook cells are now the LLM's context window!\n"
     ]
    }
   ],
   "source": [
    "# Re-enable chat mode\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Context Management\n",
    "\n",
    "You can manage how context is handled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current context strategy\n",
    "%llm_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also manually toggle notebook context mode\n",
    "%llm_notebook_context status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Debugging Support\n",
    "\n",
    "For development, you can enable debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable debugger (for VS Code)\n",
    "# %llm_debug 5678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conversation Continuation\n",
    "\n",
    "The beauty of notebook context is that your entire notebook becomes the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Based on everything we've discussed in this notebook, can you summarize the key features of the LLM Kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Tips and Tricks\n",
    "\n",
    "- In chat mode, regular Python code still works normally\n",
    "- Magic commands (starting with %) are not sent to the LLM\n",
    "- Comments (starting with #) are also not sent to the LLM\n",
    "- Empty cells are ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python comment - won't be sent to LLM\n",
    "x = 42\n",
    "print(f\"The answer is {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What was the value of x in the previous cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Disabling Chat Mode\n",
    "\n",
    "When you're done with chat mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status and disable\n",
    "%llm_chat status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%llm_chat off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The LLM Kernel provides:\n",
    "\n",
    "1. **Chat Mode** - Natural conversation without magic commands\n",
    "2. **Notebook Context** - Your cells ARE the context window\n",
    "3. **Multi-Model Support** - Switch between different LLMs\n",
    "4. **Context Visualization** - See what the LLM sees\n",
    "5. **Smart Filtering** - Ignores magic commands and comments\n",
    "6. **Debugging Support** - VS Code integration\n",
    "\n",
    "The notebook becomes a living conversation where every cell contributes to the context!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
