{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Kernel Demo: Notebook Cells ARE the Context Window\n",
    "\n",
    "This demo shows the revolutionary feature where your notebook cells literally become the LLM's context window. Every cell you execute is part of the conversation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Enable Chat Mode\n",
    "\n",
    "This activates both chat mode AND notebook context mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat mode: ON\n",
      "üìì Notebook context mode: ON\n",
      "üìù Just type in any cell to chat!\n",
      "üí° Your notebook cells are now the LLM's context window!\n"
     ]
    }
   ],
   "source": [
    "# Enable chat mode - your notebook becomes the conversation!\n",
    "%llm_chat on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Start a Conversation\n",
    "\n",
    "Now just type naturally in cells - no magic commands needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "Hello, Alice! I'm excited to help you with your Python coding journey. Feel free to ask any questions you have, and I'll do my best to assist you! What are you working on today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n",
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n",
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n"
     ]
    }
   ],
   "source": [
    "Hello! I'm going to write some Python code and ask you questions about it. My name is Alice and I'm learning Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [1, 2, 3, 4, 5]\n",
      "Squared: [1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "# Let me define some variables\n",
    "my_list = [1, 2, 3, 4, 5]\n",
    "squared = [x**2 for x in my_list]\n",
    "print(f\"Original: {my_list}\")\n",
    "print(f\"Squared: {squared}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What did I just do in the previous cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: See What the LLM Sees\n",
    "\n",
    "The magic - the LLM sees your actual notebook cells as context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìì Notebook Context Mode - Showing cells that will be sent to LLM:\n",
      "============================================================\n",
      "\n",
      "[1] USER:\n",
      "# Enable chat mode - your notebook becomes the conversation!\n",
      "%llm_chat on\n",
      "----------------------------------------\n",
      "\n",
      "[2] USER:\n",
      "# Let me define some variables\n",
      "my_list = [1, 2, 3, 4, 5]\n",
      "squared = [x**2 for x in my_list]\n",
      "print(f\"Original: {my_list}\")\n",
      "print(f\"Squared: {squared}\")\n",
      "----------------------------------------\n",
      "\n",
      "[3] USER:\n",
      "# Show the current context window\n",
      "%llm_context\n",
      "----------------------------------------\n",
      "\n",
      "Total messages: 3\n",
      "Estimated tokens: ~67\n"
     ]
    }
   ],
   "source": [
    "# Show the current context window\n",
    "%llm_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: The Context Grows with Your Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Alice! Welcome to Python programming.\n"
     ]
    }
   ],
   "source": [
    "# More code\n",
    "def greet(name):\n",
    "    return f\"Hello, {name}! Welcome to Python programming.\"\n",
    "\n",
    "message = greet(\"Alice\")\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "Since the name \"Alice\" was passed to the `greet` function in your last code, I can make the function more personalized by adding specific details or attributes. Here's a modified version of the `greet` function that includes a personalized message based on the time of day:\n",
      "\n",
      "```python\n",
      "from datetime import datetime\n",
      "\n",
      "def greet(name):\n",
      "    current_hour = datetime.now().hour\n",
      "    if current_hour < 12:\n",
      "        time_of_day = \"morning\"\n",
      "    elif current_hour < 18:\n",
      "        time_of_day = \"afternoon\"\n",
      "    else:\n",
      "        time_of_day = \"evening\"\n",
      "    \n",
      "    return f\"Good {time_of_day}, {name}! Welcome to Python programming.\"\n",
      "```\n",
      "\n",
      "Now, this function will greet the user according to the time of day! Would you like to see it in action with a specific name?\n"
     ]
    }
   ],
   "source": [
    "Do you remember my name? Can you modify the greet function to be more personalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Out-of-Order Execution Still Works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Let's define a new function\n",
    "def calculate_average(numbers):\n",
    "    return sum(numbers) / len(numbers)\n",
    "\n",
    "avg = calculate_average(my_list)\n",
    "print(f\"Average: {avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "Certainly! The `my_list` you defined earlier contains the values `[1, 2, 3, 4, 5]`. The squared version of this list, which you calculated using a list comprehension, is `[1, 4, 9, 16, 25]`.\n"
     ]
    }
   ],
   "source": [
    "I used my_list from earlier. Can you tell me what values were in it and what the squared version was?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Context Includes Everything\n",
    "\n",
    "The LLM sees:\n",
    "- Your code cells\n",
    "- Your questions\n",
    "- Previous outputs\n",
    "- The entire conversation flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìì Notebook Context Mode - Showing cells that will be sent to LLM:\n",
      "============================================================\n",
      "\n",
      "[1] USER:\n",
      "# Enable chat mode - your notebook becomes the conversation!\n",
      "%llm_chat on\n",
      "----------------------------------------\n",
      "\n",
      "[2] USER:\n",
      "# Let me define some variables\n",
      "my_list = [1, 2, 3, 4, 5]\n",
      "squared = [x**2 for x in my_list]\n",
      "print(f\"Original: {my_list}\")\n",
      "print(f\"Squared: {squared}\")\n",
      "----------------------------------------\n",
      "\n",
      "[3] USER:\n",
      "# Show the current context window\n",
      "%llm_context\n",
      "----------------------------------------\n",
      "\n",
      "[4] USER:\n",
      "# More code\n",
      "def greet(name):\n",
      "    return f\"Hello, {name}! Welcome to Python programming.\"\n",
      "\n",
      "message = greet(\"Alice\")\n",
      "print(message)\n",
      "----------------------------------------\n",
      "\n",
      "[5] USER:\n",
      "# Let's define a new function\n",
      "def calculate_average(numbers):\n",
      "    return sum(numbers) / len(numbers)\n",
      "\n",
      "avg = calculate_average(my_list)\n",
      "print(f\"Average: {avg}\")\n",
      "----------------------------------------\n",
      "\n",
      "[6] USER:\n",
      "# Let's see the full context again\n",
      "%llm_context\n",
      "----------------------------------------\n",
      "\n",
      "Total messages: 6\n",
      "Estimated tokens: ~150\n"
     ]
    }
   ],
   "source": [
    "# Let's see the full context again\n",
    "%llm_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "Sure! Here are the Python functions we've defined so far:\n",
      "\n",
      "1. **`greet(name)`**: This function takes a name as an argument and returns a greeting message.\n",
      "\n",
      "2. **`calculate_average(numbers)`**: This function takes a list of numbers as an argument and returns the average of those numbers.\n"
     ]
    }
   ],
   "source": [
    "This is amazing! The notebook IS the conversation. Can you list all the Python functions we've defined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LLM Kernel] INFO: \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "[LLM Kernel] INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[LLM Kernel] INFO: Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini]\n",
      "----------------------------------------\n",
      "Sure! Here's a summary of what we've done in this notebook:\n",
      "\n",
      "1. **Variable Definition**: We defined a list named `my_list` containing the numbers 1 through 5.\n",
      "2. **List Comprehension**: We created a new list called `squared` containing the squares of each element in `my_list`.\n",
      "   - We printed both the original list and the squared list.\n",
      "3. **Function Creation**: We defined a function named `greet` that takes a name as an argument and returns a greeting message.\n",
      "   - We then called this function with the argument \"Alice\" and printed the resulting message.\n",
      "4. **Average Calculation**: We defined another function called `calculate_average` which takes a list of numbers and returns their average.\n",
      "   - We used this function to calculate the average of `my_list` and printed the result.\n",
      "\n",
      "Let me know if there's anything else you'd like to explore!\n"
     ]
    }
   ],
   "source": [
    "Can you write a summary of everything we've done in this notebook so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Moving Cells Changes Context Order\n",
    "\n",
    "If you move cells around in your notebook, the context follows the cell order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "1. **No Separate Conversation History** - The notebook cells ARE the history\n",
    "2. **Natural Flow** - Type code or questions in any cell\n",
    "3. **Perfect Context** - The LLM sees exactly what you see\n",
    "4. **Reorderable** - Move cells to reorganize context\n",
    "5. **Transparent** - Use `%llm_context` to see exactly what the LLM sees\n",
    "\n",
    "This is a paradigm shift - your notebook becomes a living, intelligent document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off chat mode when done\n",
    "%llm_chat off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Traditional Context Mode\n",
      "Use %llm_status to see conversation history\n"
     ]
    }
   ],
   "source": [
    "%llm_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ LLM Kernel Status\n",
      "==================================================\n",
      "Active Model: gpt-4o-mini\n",
      "Available Models: 9\n",
      "Conversation History: 0 exchanges\n"
     ]
    }
   ],
   "source": [
    "%llm_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Kernel",
   "language": "python",
   "name": "llm_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
