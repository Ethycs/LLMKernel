# LLM Kernel Environment Configuration
# Copy this file to .env and fill in your API keys

# OpenAI API Key (get from https://platform.openai.com/api-keys)
OPENAI_API_KEY=sk-your-openai-key-here

# Anthropic API Key (get from https://console.anthropic.com/)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# Google API Key (get from https://console.cloud.google.com/)
GOOGLE_API_KEY=your-google-key-here

# Azure OpenAI (if using Azure)
# AZURE_OPENAI_API_KEY=your-azure-key
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/

# Ollama (if using local models)
# OLLAMA_BASE_URL=http://localhost:11434

# LiteLLM Proxy (if using)
# LITELLM_API_KEY=sk-LiteLLM-Master-Key
# LITELLM_BASE_URL=https://litellm.your-domain.com

# Default model selection
DEFAULT_LLM_MODEL=gpt-4o-mini
FALLBACK_LLM_MODEL=gpt-3.5-turbo

# Context management settings
LLM_KERNEL_CONTEXT_STRATEGY=smart
LLM_KERNEL_MAX_TOKENS=4000
LLM_KERNEL_MAX_CELLS=20
LLM_KERNEL_AUTO_PRUNE=true
LLM_KERNEL_PRUNING_STRATEGY=hybrid
LLM_KERNEL_PRUNING_THRESHOLD=0.7

# Performance settings
LLM_KERNEL_PARALLEL=true
LLM_KERNEL_TIMEOUT=30

# Logging
LLM_KERNEL_DEBUG=false
LLM_KERNEL_LOG_LEVEL=INFO
